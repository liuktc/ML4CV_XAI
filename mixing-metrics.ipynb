{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:19:01.534130Z",
     "iopub.status.busy": "2025-04-01T14:19:01.533845Z",
     "iopub.status.idle": "2025-04-01T14:19:16.615275Z",
     "shell.execute_reply": "2025-04-01T14:19:16.614249Z",
     "shell.execute_reply.started": "2025-04-01T14:19:01.534093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ML4CV_XAI'...\n",
      "remote: Enumerating objects: 818, done.\u001b[K\n",
      "remote: Counting objects: 100% (29/29), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 818 (delta 6), reused 19 (delta 5), pack-reused 789 (from 1)\u001b[K\n",
      "Receiving objects: 100% (818/818), 63.56 MiB | 39.13 MiB/s, done.\n",
      "Resolving deltas: 100% (359/359), done.\n",
      "Collecting captum\n",
      "  Downloading captum-0.8.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting grad_cam\n",
      "  Downloading grad-cam-1.5.4.tar.gz (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting Craft-xai\n",
      "  Downloading Craft_xai-0.0.3-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting torcheval\n",
      "  Downloading torcheval-0.0.7-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.5)\n",
      "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from captum) (24.2)\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from captum) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.67.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad_cam) (11.0.0)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad_cam) (0.20.1+cu121)\n",
      "Collecting ttach (from grad_cam)\n",
      "  Downloading ttach-0.0.3-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad_cam) (4.10.0.84)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad_cam) (1.2.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from Craft-xai) (0.25.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from Craft-xai) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0->captum) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0->captum) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0->captum) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0->captum) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0->captum) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0->captum) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (3.17.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->captum) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->Craft-xai) (2.36.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->Craft-xai) (2024.12.12)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->Craft-xai) (0.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad_cam) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad_cam) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0->captum) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0->captum) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0->captum) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0->captum) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0->captum) (2024.2.0)\n",
      "Downloading captum-0.8.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Craft_xai-0.0.3-py3-none-any.whl (18 kB)\n",
      "Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
      "Building wheels for collected packages: grad_cam\n",
      "  Building wheel for grad_cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for grad_cam: filename=grad_cam-1.5.4-py3-none-any.whl size=39680 sha256=0a68309590a3edf4e79646b401573e3d306ac48f301b6b3a2c46364d5a4b2a7a\n",
      "  Stored in directory: /root/.cache/pip/wheels/50/b0/82/1f97b5348c7fe9f0ce0ba18497202cafa5dec4562bd5292680\n",
      "Successfully built grad_cam\n",
      "Installing collected packages: ttach, torcheval, grad_cam, Craft-xai, captum\n",
      "Successfully installed Craft-xai-0.0.3 captum-0.8.0 grad_cam-1.5.4 torcheval-0.0.7 ttach-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./ML4CV_XAI\n",
    "!git clone https://github.com/liuktc/ML4CV_XAI.git\n",
    "!pip install captum grad_cam Craft-xai torcheval\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/ML4CV_XAI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:19:24.213544Z",
     "iopub.status.busy": "2025-04-01T14:19:24.213196Z",
     "iopub.status.idle": "2025-04-01T14:19:43.526636Z",
     "shell.execute_reply": "2025-04-01T14:19:43.525812Z",
     "shell.execute_reply.started": "2025-04-01T14:19:24.213514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import _DeepLiftShap, _GradCAMPlusPlus, SimpleUpsampling,ERFUpsamplingFast, min_max_normalize\n",
    "from data import imagenettewoof\n",
    "from results.results_metrics import ResultMetrics\n",
    "from models import vgg11_PascalVOC, vgg_preprocess, vgg11_Imagenettewoof\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:19:58.789982Z",
     "iopub.status.busy": "2025-04-01T14:19:58.789636Z",
     "iopub.status.idle": "2025-04-01T14:20:04.437048Z",
     "shell.execute_reply": "2025-04-01T14:20:04.436302Z",
     "shell.execute_reply.started": "2025-04-01T14:19:58.789957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = vgg11_Imagenettewoof()\n",
    "model.to(device)\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load('./VGG11_PascalVOC.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "preprocess = vgg_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:20:08.463650Z",
     "iopub.status.busy": "2025-04-01T14:20:08.463355Z",
     "iopub.status.idle": "2025-04-01T14:21:21.942468Z",
     "shell.execute_reply": "2025-04-01T14:21:21.941733Z",
     "shell.execute_reply.started": "2025-04-01T14:20:08.463631Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\imagewoof2-320\n",
      "data\\imagewoof2-320\n"
     ]
    }
   ],
   "source": [
    "train_data = imagenettewoof(split=\"train\",size=\"320px\",download=False, transform=preprocess)\n",
    "test_data = imagenettewoof(split=\"test\",size=\"320px\", download=False, transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "## TO CHANGE IN KAGGLE\n",
    "INDEX = 0 # 0,1,2,3\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "###########################################\n",
    "\n",
    "# Number of parts to split the dataset into\n",
    "num_parts = 4\n",
    "\n",
    "# Get the total number of images in the dataset\n",
    "total_size = len(test_data)\n",
    "\n",
    "# Split the dataset indices into roughly equal parts\n",
    "split_indices = np.array_split(np.arange(total_size), num_parts)\n",
    "\n",
    "INDICES = split_indices[INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:21:26.817380Z",
     "iopub.status.busy": "2025-04-01T14:21:26.817069Z",
     "iopub.status.idle": "2025-04-01T14:21:26.889701Z",
     "shell.execute_reply": "2025-04-01T14:21:26.888943Z",
     "shell.execute_reply.started": "2025-04-01T14:21:26.817359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# BATCH_SIZE_TEST = 1\n",
    "\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "NUM_TRAIN = 16\n",
    "\n",
    "# dl_test = DataLoader(test_data, batch_size=BATCH_SIZE_TEST, shuffle=False)\n",
    "dl_train = DataLoader(Subset(train_data, torch.randperm(len(train_data))[:NUM_TRAIN]), batch_size=BATCH_SIZE_TRAIN, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:21:28.343473Z",
     "iopub.status.busy": "2025-04-01T14:21:28.343186Z",
     "iopub.status.idle": "2025-04-01T14:21:28.511033Z",
     "shell.execute_reply": "2025-04-01T14:21:28.510291Z",
     "shell.execute_reply.started": "2025-04-01T14:21:28.343452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_dist_16 = torch.cat([images for images, _ in dl_train], dim=0).to(device)\n",
    "baseline_dist_8 = baseline_dist_16[:8].clone()\n",
    "baseline_dist_4 = baseline_dist_16[:4].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:21:29.331317Z",
     "iopub.status.busy": "2025-04-01T14:21:29.331004Z",
     "iopub.status.idle": "2025-04-01T14:21:29.394992Z",
     "shell.execute_reply": "2025-04-01T14:21:29.394049Z",
     "shell.execute_reply.started": "2025-04-01T14:21:29.331294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LAYERS = [20,15,10,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T14:27:06.598608Z",
     "iopub.status.busy": "2025-04-01T14:27:06.598277Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results file not found. Creating new results file ./results_mixed_infidelity.csv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097a833cb20e420e8d68837302e7331a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m mixed_attribution_map \u001b[38;5;241m=\u001b[39m mix(attributions_per_layer)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m [Infidelity()]:\n\u001b[1;32m---> 59\u001b[0m     metric_result_normal \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43msaliency_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribution_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattribution_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribution_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     results\u001b[38;5;241m.\u001b[39madd_result(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG11\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     68\u001b[0m                        attribution_method\u001b[38;5;241m=\u001b[39mattribution_method\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m     69\u001b[0m                        dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagenettewoof\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m                        label\u001b[38;5;241m=\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     77\u001b[0m                        predicted_label\u001b[38;5;241m=\u001b[39mpredicted_label[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     79\u001b[0m     metric_result_mixed \u001b[38;5;241m=\u001b[39m metric(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     80\u001b[0m             test_images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m     81\u001b[0m             saliency_maps\u001b[38;5;241m=\u001b[39mmixed_attribution_map,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     85\u001b[0m             layer\u001b[38;5;241m=\u001b[39mlayer)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\Desktop\\MagistraleAI\\2nd_year\\Artificial Intelligence in Industry\\project\\metrics\\infidelity.py:36\u001b[0m, in \u001b[0;36mInfidelity.__call__\u001b[1;34m(self, model, test_images, saliency_maps, class_idx, attribution_method, device, apply_softmax, return_mean, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m test_images \u001b[38;5;241m=\u001b[39m test_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     35\u001b[0m class_idx \u001b[38;5;241m=\u001b[39m class_idx\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 36\u001b[0m res \u001b[38;5;241m=\u001b[39m infidelity(\n\u001b[0;32m     37\u001b[0m     model, perturb_fn, test_images, saliency_maps, target\u001b[38;5;241m=\u001b[39mclass_idx\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_mean:\n\u001b[0;32m     41\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_core\\infidelity.py:559\u001b[0m, in \u001b[0;36minfidelity\u001b[1;34m(forward_func, perturb_func, inputs, attributions, baselines, additional_forward_args, target, n_perturb_samples, max_examples_per_batch, normalize)\u001b[0m\n\u001b[0;32m    555\u001b[0m bsz \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# if not normalize, directly return aggrgated MSE ((a-b)^2,)\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# else return aggregated MSE's polynomial expansion tensors (a^2, ab, b^2)\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     agg_tensors \u001b[38;5;241m=\u001b[39m \u001b[43m_divide_and_aggregate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTuple\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_perturb_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_next_infidelity_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43magg_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_sum_infidelity_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_examples_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_examples_per_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normalize:\n\u001b[0;32m    568\u001b[0m     beta_num \u001b[38;5;241m=\u001b[39m agg_tensors[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_utils\\batching.py:71\u001b[0m, in \u001b[0;36m_divide_and_aggregate_metrics\u001b[1;34m(inputs, n_perturb_samples, metric_func, agg_func, max_examples_per_batch)\u001b[0m\n\u001b[0;32m     63\u001b[0m max_inps_per_batch \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     n_perturb_samples\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_examples_per_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(max_examples_per_batch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m bsz, \u001b[38;5;241m1\u001b[39m), n_perturb_samples)\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m current_n_steps \u001b[38;5;241m=\u001b[39m max_inps_per_batch\n\u001b[1;32m---> 71\u001b[0m metrics_sum \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_inps_per_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_n_steps \u001b[38;5;241m<\u001b[39m n_perturb_samples:\n\u001b[0;32m     74\u001b[0m     current_n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m max_inps_per_batch\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\metrics\\_core\\infidelity.py:486\u001b[0m, in \u001b[0;36minfidelity.<locals>._next_infidelity_tensors\u001b[1;34m(current_n_perturb_samples)\u001b[0m\n\u001b[0;32m    475\u001b[0m targets_expanded \u001b[38;5;241m=\u001b[39m _expand_target(\n\u001b[0;32m    476\u001b[0m     target,\n\u001b[0;32m    477\u001b[0m     current_n_perturb_samples,\n\u001b[0;32m    478\u001b[0m     expansion_type\u001b[38;5;241m=\u001b[39mExpansionTypes\u001b[38;5;241m.\u001b[39mrepeat_interleave,\n\u001b[0;32m    479\u001b[0m )\n\u001b[0;32m    480\u001b[0m additional_forward_args_expanded \u001b[38;5;241m=\u001b[39m _expand_additional_forward_args(\n\u001b[0;32m    481\u001b[0m     additional_forward_args,\n\u001b[0;32m    482\u001b[0m     current_n_perturb_samples,\n\u001b[0;32m    483\u001b[0m     expansion_type\u001b[38;5;241m=\u001b[39mExpansionTypes\u001b[38;5;241m.\u001b[39mrepeat_interleave,\n\u001b[0;32m    484\u001b[0m )\n\u001b[1;32m--> 486\u001b[0m inputs_perturbed_fwd \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_perturbed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets_expanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args_expanded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m inputs_fwd \u001b[38;5;241m=\u001b[39m _run_forward(forward_func, inputs, target, additional_forward_args)\n\u001b[0;32m    493\u001b[0m inputs_fwd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(\n\u001b[0;32m    494\u001b[0m     inputs_fwd, current_n_perturb_samples, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    495\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\captum\\_utils\\common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from metrics import RoadCombined, Infidelity, Sensitivity, AverageDrop, InsertionCurveAUC, DeletionCurveAUC\n",
    "from utils import MultiplierMix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "results = ResultMetrics(\"./results_mixed_infidelity.csv\")\n",
    "\n",
    "last_image_index = results.get_last_image_index()\n",
    "\n",
    "for index in tqdm(INDICES):\n",
    "    if index < last_image_index:\n",
    "        continue\n",
    "    images, labels = test_data[index]\n",
    "    images = images.unsqueeze(0)  # Add batch dimension\n",
    "    labels = torch.Tensor([labels]).long()  # Convert to long tensor    \n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device).reshape(-1)\n",
    "\n",
    "    predicted_label = model(images).argmax(dim=1)\n",
    "\n",
    "    # Make sure that LAYERS is sorted in descending order\n",
    "    LAYERS.sort(reverse=True)\n",
    "    # for attribution_method in [_DeepLiftShap(baseline_dist_8, name=\"DeepLiftShap8\"), _GradCAMPlusPlus()]:\n",
    "    for attribution_method in [_GradCAMPlusPlus()]:   \n",
    "        for upsample_method in [ERFUpsamplingFast(), SimpleUpsampling((224,224))]:\n",
    "            attributions_per_layer = []\n",
    "            for layer_index in LAYERS:\n",
    "                layer = model.features[layer_index]\n",
    "                attribution_map = attribution_method.attribute(input_tensor=images,\n",
    "                                                                model=model,\n",
    "                                                                layer=layer,\n",
    "                                                                target=labels\n",
    "                                                                )\n",
    "\n",
    "                # upsample_method = SimpleUpsampling((224,224))\n",
    "                attribution_map = upsample_method(attribution=attribution_map,\n",
    "                                                  image=images,\n",
    "                                                  device=device,\n",
    "                                                  model=model,\n",
    "                                                  layer=layer)\n",
    "\n",
    "                if (torch.abs(\n",
    "                    attribution_map.amax(dim=(2, 3), keepdim=True)\n",
    "                    - attribution_map.amin(dim=(2, 3), keepdim=True)\n",
    "                )\n",
    "                < 1e-6  ).any():\n",
    "                    print(\"A saliency map is constant, skipping batch\")\n",
    "                    del images, labels, attribution_map\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "                attribution_map = min_max_normalize(attribution_map)\n",
    "                attributions_per_layer.append(attribution_map)\n",
    "\n",
    "                # Mix the attribution maps\n",
    "                mix = MultiplierMix(layers_to_combine=\"all\")\n",
    "                mixed_attribution_map = mix(attributions_per_layer)\n",
    "\n",
    "                for metric in [Infidelity()]:\n",
    "                    metric_result_normal = metric(model=model,\n",
    "                            test_images=images,\n",
    "                            saliency_maps=attribution_map,\n",
    "                            class_idx=labels,\n",
    "                            attribution_method=attribution_method,\n",
    "                            device=device,\n",
    "                            layer=layer)\n",
    "                    \n",
    "                    results.add_result(model=\"VGG11\",\n",
    "                                       attribution_method=attribution_method.name,\n",
    "                                       dataset=\"Imagenettewoof\",\n",
    "                                       layer=f\"features.{layer_index}\",\n",
    "                                       metric=metric.name,\n",
    "                                       upscale_method=upsample_method.name,\n",
    "                                       mixing_method= \"None\",\n",
    "                                       value=metric_result_normal,\n",
    "                                       image_index=index,\n",
    "                                       label=labels[0].item(),\n",
    "                                       predicted_label=predicted_label[0].item())\n",
    "\n",
    "                    metric_result_mixed = metric(model=model,\n",
    "                            test_images=images,\n",
    "                            saliency_maps=mixed_attribution_map,\n",
    "                            class_idx=labels,\n",
    "                            attribution_method=attribution_method,\n",
    "                            device=device,\n",
    "                            layer=layer)\n",
    "                    \n",
    "                    \n",
    "                    results.add_result(model=\"VGG11\",\n",
    "                                       attribution_method=attribution_method.name,\n",
    "                                       dataset=\"Imagenettewoof\",\n",
    "                                       layer=f\"features.{layer_index}\",\n",
    "                                       metric=metric.name,\n",
    "                                       upscale_method=upsample_method.name,\n",
    "                                       mixing_method= mix.name,\n",
    "                                       value=metric_result_mixed,\n",
    "                                       image_index=index,\n",
    "                                       label=labels[0].item(),\n",
    "                                       predicted_label=predicted_label[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 240133,
     "modelInstanceId": 218406,
     "sourceId": 255459,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
