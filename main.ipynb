{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils import _DeepLiftShap, _GradCAMPlusPlus\n",
    "from data import PascalVOC2007\n",
    "from results.results_metrics import ResultMetrics\n",
    "from models import vgg11_PascalVOC\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the VGG-11 model is instantiated, the last layer is replaced with a new layer that has 20 outputs instead of 1000 (because PascalVOC has 20 classes, but the model was trained on ImageNet which has 1000 classes). The loaded weights are pretrained on ImageNet and the fine-tuned on PascalVOC. For more detail about the fine-tuning process, see `fine_tuning.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg11_PascalVOC()\n",
    "model.to(device)\n",
    "# Load the pretrained weights\n",
    "model.load_state_dict(torch.load('VGG11_PascalVOC.pt', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),                 # Convert to Tensor\n",
    "    transforms.Normalize(                  # Normalize using ImageNet mean and std\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (14): ReLU(inplace=True)\n",
       "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (1): ReLU(inplace=True)\n",
       "   (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (4): ReLU(inplace=True)\n",
       "   (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (7): ReLU(inplace=True)\n",
       "   (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (9): ReLU(inplace=True)\n",
       "   (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (12): ReLU(inplace=True)\n",
       "   (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (14): ReLU(inplace=True)\n",
       "   (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (17): ReLU(inplace=True)\n",
       "   (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (19): ReLU(inplace=True)\n",
       "   (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(7, 7)),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "   (1): ReLU(inplace=True)\n",
       "   (2): Dropout(p=0.5, inplace=False)\n",
       "   (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "   (4): ReLU(inplace=True)\n",
       "   (5): Dropout(p=0.5, inplace=False)\n",
       "   (6): Linear(in_features=4096, out_features=20, bias=True)\n",
       " )]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-5        [-1, 128, 112, 112]               0\n",
      "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
      "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
      "              ReLU-8          [-1, 256, 56, 56]               0\n",
      "            Conv2d-9          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-10          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-11          [-1, 256, 28, 28]               0\n",
      "           Conv2d-12          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-13          [-1, 512, 28, 28]               0\n",
      "           Conv2d-14          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-15          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-16          [-1, 512, 14, 14]               0\n",
      "           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-18          [-1, 512, 14, 14]               0\n",
      "           Conv2d-19          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-20          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-21            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-22            [-1, 512, 7, 7]               0\n",
      "           Linear-23                 [-1, 4096]     102,764,544\n",
      "             ReLU-24                 [-1, 4096]               0\n",
      "          Dropout-25                 [-1, 4096]               0\n",
      "           Linear-26                 [-1, 4096]      16,781,312\n",
      "             ReLU-27                 [-1, 4096]               0\n",
      "          Dropout-28                 [-1, 4096]               0\n",
      "           Linear-29                   [-1, 20]          81,940\n",
      "================================================================\n",
      "Total params: 128,848,276\n",
      "Trainable params: 128,848,276\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 125.37\n",
      "Params size (MB): 491.52\n",
      "Estimated Total Size (MB): 617.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: data\\VOCtest_06-Nov-2007.tar\n",
      "Extracting data\\VOCtest_06-Nov-2007.tar to data\n",
      "Using downloaded and verified file: data\\VOCtrainval_06-Nov-2007.tar\n",
      "Extracting data\\VOCtrainval_06-Nov-2007.tar to data\n"
     ]
    }
   ],
   "source": [
    "test_data = PascalVOC2007(\"test\", transform=preprocess)\n",
    "train_data = PascalVOC2007(\"trainval\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train data is only used in the case of DeepLiftSHAP, because it requires a baseline distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "BATCH_SIZE_TEST = 2\n",
    "NUM_TEST = 1024\n",
    "\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "NUM_TRAIN = 8\n",
    "\n",
    "dl_test = DataLoader(Subset(test_data, torch.randperm(len(test_data))[:NUM_TEST]), batch_size=BATCH_SIZE_TEST, shuffle=False)\n",
    "dl_train = DataLoader(Subset(train_data, torch.randperm(len(train_data))[:NUM_TRAIN]), batch_size=BATCH_SIZE_TRAIN, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of all the attributions is equal to the difference between the output of the model on the current input, minus the average output of the model on the baseline distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The parameter `attribution_method` is an object that extends the base class `AttributionMethod` and implements the method `attribute`. In this way, simply changing the object passed to the `explain` method, we can change the attribution method used.\n",
    "- The parameter `train_dl` is only used by the DeepLiftSHAP method, because it requires a baseline distribution.\n",
    "- The function automatically add the metrics to the `ResultMetrics` object passed as parameter.\n",
    "- The boolean `rescale_saliency` is used to rescale the saliency map in such a way that the integral of the saliency map is equal to $r \\times (H \\times W)$, where $H$ and $W$ are the height and width of the saliency map, and $r \\in [0, 1]$ is the rescale factor.\n",
    "    - This is useful to compare different saliency maps methods, because, for example, producing a saliency map with all ones is not useful, but gives the best results for some of the metrics.\n",
    "    - The scaling is done in such a way that the positions and the values where the saliency map is equal to one are not changed. More formally, given the saliency map $S(i, j): \\mathbb{R}^2 \\rightarrow [0,1]$, the rescaled saliency map $S'(i, j) = S(i,j)^{\\alpha^2}$ is found as an optimization problem by defining:\n",
    "\n",
    "    $$L(S(i,j), \\alpha) = \\left( \\sum_{i,j} S(i,j)^{\\alpha^2}  - (r \\times H \\times W) \\right)^2$$\n",
    "\n",
    "    $$\\alpha = \\arg \\min_{\\alpha \\in \\mathbb{R}} L(S(i,j), \\alpha)$$\n",
    "\n",
    "    - It can be seen as a way to constraint the saliency map into keeping only a limited amount of important pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLiftShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.json\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "calculate_metrics(model,\n",
    "                _DeepLiftShap(),\n",
    "                dl_test,\n",
    "                dl_train,\n",
    "                layers=[f for f in model.features],\n",
    "                result_metrics=results,\n",
    "                device=device,\n",
    "                upsample=\"bilinear\",\n",
    "                rescale_saliency=False,\n",
    "                model_name=\"VGG11\",\n",
    "                debug=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLiftShap with rescaling ($r=0.3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.json\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "calculate_metrics(model,\n",
    "                _DeepLiftShap(),\n",
    "                dl_test,\n",
    "                dl_train,\n",
    "                layers=[f for f in model.features],\n",
    "                result_metrics=results,\n",
    "                device=device,\n",
    "                upsample=\"bilinear\",\n",
    "                rescale_saliency=True,\n",
    "                model_name=\"VGG11_Rescaling\",\n",
    "                debug=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCam++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.json\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "\n",
    "for f in model.features:\n",
    "    attr = _GradCAMPlusPlus(model, f)\n",
    "    calculate_metrics(model,\n",
    "                    attr,\n",
    "                    dl_test,\n",
    "                    dl_train,\n",
    "                    layers=[f],\n",
    "                    result_metrics=results,\n",
    "                    device=device,\n",
    "                    upsample=\"bilinear\",\n",
    "                    rescale_saliency=False,\n",
    "                    model_name=\"VGG11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCam++ with rescaling ($r=0.3$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import calculate_metrics\n",
    "\n",
    "RESULT_PATH = \"./results.json\"\n",
    "\n",
    "results = ResultMetrics(RESULT_PATH)\n",
    "\n",
    "for f in model.features:\n",
    "    attr = _GradCAMPlusPlus(model, f)\n",
    "    calculate_metrics(model,\n",
    "                    attr,\n",
    "                    dl_test,\n",
    "                    dl_train,\n",
    "                    layers=[f],\n",
    "                    result_metrics=results,\n",
    "                    device=device,\n",
    "                    upsample=\"bilinear\",\n",
    "                    rescale_saliency=True,\n",
    "                    model_name=\"VGG11_Rescaling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
